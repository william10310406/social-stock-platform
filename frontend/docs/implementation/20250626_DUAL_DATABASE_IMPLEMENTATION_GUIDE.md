# Stock Insight Platform - é›™è³‡æ–™åº«å¯¦æ–½æŒ‡å—

## ğŸ¯ æ¦‚è¦½

æœ¬æŒ‡å—è©³ç´°èªªæ˜å¦‚ä½•åœ¨ Stock Insight Platform ä¸­å¯¦æ–½å†·ç†±æ•¸æ“šåˆ†é›¢å‹é›™è³‡æ–™åº«æ¶æ§‹ã€‚

### ğŸ“Š æ¶æ§‹æ¦‚è¿°
- **ç†±è³‡æ–™åº«**: MSSQL Server 2022 (é«˜æ€§èƒ½å³æ™‚æ•¸æ“š)
- **å†·è³‡æ–™åº«**: PostgreSQL 14 (æ­·å²æ•¸æ“šå’Œåˆ†æ)
- **åˆ†é›¢ç­–ç•¥**: ä»¥30å¤©ç‚ºç•Œé™é€²è¡Œå†·ç†±æ•¸æ“šåˆ†é›¢

---

## ğŸš€ Phase 1: åŸºç¤è¨­æ–½é…ç½®

### 1.1 Docker Compose æ“´å±•

æ›´æ–° `docker-compose.yml` æ·»åŠ å†·è³‡æ–™åº«ï¼š

```yaml
services:
  # ç¾æœ‰çš„ç†±è³‡æ–™åº« (é‡å‘½å)
  hot-db:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: stock-insight-hot-db
    ports:
      - "1433:1433"
    env_file:
      - .env
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD}
      - MSSQL_PID=Express
    volumes:
      - hot_db_data:/var/opt/mssql
      - ./mssql_backup:/mssql_backup
    networks:
      - stock-insight-net
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'

  # æ–°å¢çš„å†·è³‡æ–™åº«
  cold-db:
    image: postgres:14
    container_name: stock-insight-cold-db
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_COLD_DATABASE}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - cold_db_data:/var/lib/postgresql/data
      - ./postgres_backup:/postgres_backup
    networks:
      - stock-insight-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

volumes:
  hot_db_data:
  cold_db_data:
```

### 1.2 ç’°å¢ƒè®Šæ•¸é…ç½®

æ›´æ–° `.env` æ–‡ä»¶ï¼š

```bash
# ç†±è³‡æ–™åº« (MSSQL)
MSSQL_HOST=stock-insight-hot-db
MSSQL_PORT=1433
MSSQL_HOT_DATABASE=StockInsight_Hot
MSSQL_USER=sa
MSSQL_SA_PASSWORD=YourStrongPassword123!

# å†·è³‡æ–™åº« (PostgreSQL)
POSTGRES_HOST=stock-insight-cold-db
POSTGRES_PORT=5432
POSTGRES_COLD_DATABASE=StockInsight_Cold
POSTGRES_USER=postgres
POSTGRES_PASSWORD=YourColdDbPassword123!

# æ•¸æ“šåº«é…ç½®ç’°å¢ƒ
FLASK_CONFIG=dual_database
```

---

## ğŸ”§ Phase 2: Flask é…ç½®æ›´æ–°

### 2.1 é…ç½®é¡æ“´å±•

æ›´æ–° `backend/app/config.py`ï¼š

```python
class DualDatabaseConfig(Config):
    """é›™è³‡æ–™åº«é…ç½®"""
    DEBUG = True
    
    # ä¸»è³‡æ–™åº« (ç†±è³‡æ–™åº« - MSSQL)
    SQLALCHEMY_DATABASE_URI = (
        f"mssql+pyodbc://{os.environ.get('MSSQL_USER', 'sa')}:"
        f"{os.environ.get('MSSQL_SA_PASSWORD')}@"
        f"{os.environ.get('MSSQL_HOST', 'localhost')}:"
        f"{os.environ.get('MSSQL_PORT', '1433')}/"
        f"{os.environ.get('MSSQL_HOT_DATABASE', 'StockInsight_Hot')}"
        f"?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes"
    )
    
    # æ•¸æ“šåº«ç¶å®šé…ç½®
    SQLALCHEMY_BINDS = {
        'hot': SQLALCHEMY_DATABASE_URI,  # ç†±è³‡æ–™åº«
        'cold': (
            f"postgresql://{os.environ.get('POSTGRES_USER', 'postgres')}:"
            f"{os.environ.get('POSTGRES_PASSWORD')}@"
            f"{os.environ.get('POSTGRES_HOST', 'localhost')}:"
            f"{os.environ.get('POSTGRES_PORT', '5432')}/"
            f"{os.environ.get('POSTGRES_COLD_DATABASE', 'StockInsight_Cold')}"
        )
    }
    
    # é€£æ¥æ± é…ç½®
    SQLALCHEMY_ENGINE_OPTIONS = {
        'pool_size': 20,
        'max_overflow': 30,
        'pool_timeout': 30,
        'pool_recycle': 3600,
        'pool_pre_ping': True
    }
    
    # ç¶å®šç‰¹å®šé…ç½®
    SQLALCHEMY_BINDS_ENGINE_OPTIONS = {
        'cold': {
            'pool_size': 10,
            'max_overflow': 20,
            'pool_timeout': 60,
            'pool_recycle': 7200,
            'pool_pre_ping': True
        }
    }

# æ›´æ–°é…ç½®å­—å…¸
config = {
    "development": DevelopmentConfig,
    "development_mssql": DevelopmentMSSQLConfig,
    "dual_database": DualDatabaseConfig,  # æ–°å¢
    "production": ProductionConfig,
    "testing": TestingConfig,
    "default": DevelopmentConfig,
}
```

### 2.2 æ¨¡å‹ç¶å®šè¨­ç½®

å‰µå»ºæ–°çš„æ¨¡å‹æ–‡ä»¶ `backend/app/models_cold.py`ï¼š

```python
"""å†·è³‡æ–™åº«æ¨¡å‹å®šç¾©"""
from datetime import datetime
from .extensions import db


class MessageArchive(db.Model):
    """èŠå¤©è¨˜éŒ„æ­·å²æª”æ¡ˆ"""
    __bind_key__ = 'cold'
    __tablename__ = 'messages_archive'
    
    id = db.Column(db.Integer, primary_key=True)
    original_id = db.Column(db.Integer, nullable=False)  # åŸå§‹ç†±åº«ID
    conversation_id = db.Column(db.Integer, nullable=False)
    sender_id = db.Column(db.Integer, nullable=False)
    content = db.Column(db.Text, nullable=False)
    created_at = db.Column(db.DateTime, nullable=False)
    archived_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    def to_dict(self):
        return {
            'id': self.original_id,  # è¿”å›åŸå§‹IDä¿æŒä¸€è‡´æ€§
            'conversation_id': self.conversation_id,
            'sender_id': self.sender_id,
            'content': self.content,
            'created_at': self.created_at.isoformat(),
            'from_archive': True
        }


class PostArchive(db.Model):
    """è²¼æ–‡æ­·å²æª”æ¡ˆ"""
    __bind_key__ = 'cold'
    __tablename__ = 'posts_archive'
    
    id = db.Column(db.Integer, primary_key=True)
    original_id = db.Column(db.Integer, nullable=False)
    author_id = db.Column(db.Integer, nullable=False)
    title = db.Column(db.String(200), nullable=False)
    body = db.Column(db.Text, nullable=False)
    created_at = db.Column(db.DateTime, nullable=False)
    archived_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    def to_dict(self):
        return {
            'id': self.original_id,
            'author_id': self.author_id,
            'title': self.title,
            'body': self.body,
            'created_at': self.created_at.isoformat(),
            'from_archive': True
        }


class StockPriceHistory(db.Model):
    """è‚¡åƒ¹æ­·å²æ•¸æ“š"""
    __bind_key__ = 'cold'
    __tablename__ = 'stock_prices_history'
    
    id = db.Column(db.Integer, primary_key=True)
    original_id = db.Column(db.Integer, nullable=False)
    stock_id = db.Column(db.Integer, nullable=False)
    trade_date = db.Column(db.Date, nullable=False)
    open_price = db.Column(db.Numeric(10, 2))
    high_price = db.Column(db.Numeric(10, 2))
    low_price = db.Column(db.Numeric(10, 2))
    close_price = db.Column(db.Numeric(10, 2))
    change_amount = db.Column(db.Numeric(10, 2))
    volume = db.Column(db.BigInteger)
    turnover = db.Column(db.BigInteger)
    transaction_count = db.Column(db.Integer)
    created_at = db.Column(db.DateTime, nullable=False)
    archived_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    # å”¯ä¸€ç´„æŸ
    __table_args__ = (
        db.UniqueConstraint('stock_id', 'trade_date', name='uq_stock_date_archive'),
    )
    
    def to_dict(self):
        return {
            'id': self.original_id,
            'stock_id': self.stock_id,
            'trade_date': self.trade_date.isoformat(),
            'open_price': float(self.open_price) if self.open_price else None,
            'high_price': float(self.high_price) if self.high_price else None,
            'low_price': float(self.low_price) if self.low_price else None,
            'close_price': float(self.close_price) if self.close_price else None,
            'change_amount': float(self.change_amount) if self.change_amount else None,
            'volume': self.volume,
            'turnover': self.turnover,
            'transaction_count': self.transaction_count,
            'from_archive': True
        }


class UserBehaviorAnalytics(db.Model):
    """ç”¨æˆ¶è¡Œç‚ºåˆ†ææ•¸æ“š"""
    __bind_key__ = 'cold'
    __tablename__ = 'user_behavior_analytics'
    
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, nullable=False)
    action_type = db.Column(db.String(50), nullable=False)  # login, post, chat, etc.
    action_count = db.Column(db.Integer, default=1)
    analysis_date = db.Column(db.Date, nullable=False)
    metadata = db.Column(db.JSON)  # é¡å¤–çš„åˆ†ææ•¸æ“š
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    
    def to_dict(self):
        return {
            'id': self.id,
            'user_id': self.user_id,
            'action_type': self.action_type,
            'action_count': self.action_count,
            'analysis_date': self.analysis_date.isoformat(),
            'metadata': self.metadata,
            'created_at': self.created_at.isoformat()
        }
```

---

## ğŸ”„ Phase 3: æ•¸æ“šé·ç§»è…³æœ¬

### 3.1 å‰µå»ºé·ç§»è…³æœ¬

å‰µå»º `backend/scripts/data_archival.py`ï¼š

```python
#!/usr/bin/env python3
"""
æ•¸æ“šæ­¸æª”è…³æœ¬ - å°‡ç†±è³‡æ–™åº«ä¸­çš„æ­·å²æ•¸æ“šé·ç§»åˆ°å†·è³‡æ–™åº«
"""
import os
import sys
from datetime import datetime, timedelta
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import create_app
from app.extensions import db
from app.models import Message, Post, StockPrice
from app.models_cold import MessageArchive, PostArchive, StockPriceHistory


class DataArchival:
    def __init__(self):
        self.app = create_app('dual_database')
        self.app_context = self.app.app_context()
        self.cutoff_date = datetime.utcnow() - timedelta(days=30)
    
    def __enter__(self):
        self.app_context.push()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.app_context.pop()
    
    def archive_messages(self):
        """æ­¸æª”èŠå¤©è¨˜éŒ„"""
        print(f"ğŸ”„ é–‹å§‹æ­¸æª”èŠå¤©è¨˜éŒ„ (æ—©æ–¼ {self.cutoff_date.date()})")
        
        # æŸ¥è©¢éœ€è¦æ­¸æª”çš„è¨˜éŒ„
        old_messages = Message.query.filter(
            Message.created_at < self.cutoff_date
        ).all()
        
        archived_count = 0
        for msg in old_messages:
            # æª¢æŸ¥æ˜¯å¦å·²æ­¸æª”
            existing = MessageArchive.query.filter_by(
                original_id=msg.id
            ).first()
            
            if not existing:
                # å‰µå»ºæ­¸æª”è¨˜éŒ„
                archive = MessageArchive(
                    original_id=msg.id,
                    conversation_id=msg.conversation_id,
                    sender_id=msg.sender_id,
                    content=msg.content,
                    created_at=msg.created_at
                )
                db.session.add(archive)
                archived_count += 1
        
        # æäº¤æ­¸æª”
        db.session.commit()
        
        if archived_count > 0:
            # åˆªé™¤å·²æ­¸æª”çš„ç†±æ•¸æ“š
            Message.query.filter(
                Message.created_at < self.cutoff_date
            ).delete()
            db.session.commit()
        
        print(f"âœ… å®ŒæˆèŠå¤©è¨˜éŒ„æ­¸æª”: {archived_count} æ¢è¨˜éŒ„")
        return archived_count
    
    def archive_posts(self):
        """æ­¸æª”è²¼æ–‡"""
        print(f"ğŸ”„ é–‹å§‹æ­¸æª”è²¼æ–‡ (æ—©æ–¼ {self.cutoff_date.date()})")
        
        old_posts = Post.query.filter(
            Post.created_at < self.cutoff_date
        ).all()
        
        archived_count = 0
        for post in old_posts:
            existing = PostArchive.query.filter_by(
                original_id=post.id
            ).first()
            
            if not existing:
                archive = PostArchive(
                    original_id=post.id,
                    author_id=post.author_id,
                    title=post.title,
                    body=post.body,
                    created_at=post.created_at
                )
                db.session.add(archive)
                archived_count += 1
        
        db.session.commit()
        
        if archived_count > 0:
            # æ³¨æ„ï¼šéœ€è¦è™•ç†é—œè¯çš„è©•è«–å’ŒæŒ‰è®š
            Post.query.filter(
                Post.created_at < self.cutoff_date
            ).delete()
            db.session.commit()
        
        print(f"âœ… å®Œæˆè²¼æ–‡æ­¸æª”: {archived_count} æ¢è¨˜éŒ„")
        return archived_count
    
    def archive_stock_prices(self):
        """æ­¸æª”è‚¡åƒ¹æ•¸æ“š"""
        print(f"ğŸ”„ é–‹å§‹æ­¸æª”è‚¡åƒ¹æ•¸æ“š (æ—©æ–¼ {self.cutoff_date.date()})")
        
        old_prices = StockPrice.query.filter(
            StockPrice.trade_date < self.cutoff_date.date()
        ).all()
        
        archived_count = 0
        for price in old_prices:
            existing = StockPriceHistory.query.filter_by(
                original_id=price.id
            ).first()
            
            if not existing:
                archive = StockPriceHistory(
                    original_id=price.id,
                    stock_id=price.stock_id,
                    trade_date=price.trade_date,
                    open_price=price.open_price,
                    high_price=price.high_price,
                    low_price=price.low_price,
                    close_price=price.close_price,
                    change_amount=price.change_amount,
                    volume=price.volume,
                    turnover=price.turnover,
                    transaction_count=price.transaction_count,
                    created_at=price.created_at
                )
                db.session.add(archive)
                archived_count += 1
        
        db.session.commit()
        
        if archived_count > 0:
            StockPrice.query.filter(
                StockPrice.trade_date < self.cutoff_date.date()
            ).delete()
            db.session.commit()
        
        print(f"âœ… å®Œæˆè‚¡åƒ¹æ•¸æ“šæ­¸æª”: {archived_count} æ¢è¨˜éŒ„")
        return archived_count
    
    def run_full_archival(self):
        """åŸ·è¡Œå®Œæ•´æ­¸æª”"""
        print("ğŸš€ é–‹å§‹åŸ·è¡Œæ•¸æ“šæ­¸æª”æµç¨‹")
        start_time = datetime.utcnow()
        
        try:
            total_archived = 0
            total_archived += self.archive_messages()
            total_archived += self.archive_posts()
            total_archived += self.archive_stock_prices()
            
            duration = datetime.utcnow() - start_time
            print(f"ğŸ‰ æ­¸æª”å®Œæˆ! ç¸½è¨ˆæ­¸æª” {total_archived} æ¢è¨˜éŒ„ï¼Œè€—æ™‚ {duration}")
            
            return True
        except Exception as e:
            print(f"âŒ æ­¸æª”éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
            db.session.rollback()
            return False


def main():
    """ä¸»å‡½æ•¸"""
    with DataArchival() as archival:
        success = archival.run_full_archival()
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
```

### 3.2 å‰µå»ºè‡ªå‹•åŒ–å®šæ™‚ä»»å‹™

å‰µå»º `scripts/setup_archival_cron.sh`ï¼š

```bash
#!/bin/bash
"""
è¨­ç½®æ•¸æ“šæ­¸æª”å®šæ™‚ä»»å‹™
"""

set -e

echo "ğŸ”§ è¨­ç½®æ•¸æ“šæ­¸æª”å®šæ™‚ä»»å‹™..."

# å‰µå»ºæ—¥èªŒç›®éŒ„
mkdir -p /app/logs/archival

# è¨­ç½® cron ä»»å‹™ (æ¯æ—¥å‡Œæ™¨ 2:00 åŸ·è¡Œ)
CRON_JOB="0 2 * * * cd /app && python backend/scripts/data_archival.py >> /app/logs/archival/archival.log 2>&1"

# æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
if crontab -l 2>/dev/null | grep -q "data_archival.py"; then
    echo "âš ï¸  å®šæ™‚ä»»å‹™å·²å­˜åœ¨"
else
    # æ·»åŠ åˆ° crontab
    (crontab -l 2>/dev/null; echo "$CRON_JOB") | crontab -
    echo "âœ… å®šæ™‚ä»»å‹™å·²è¨­ç½®: æ¯æ—¥å‡Œæ™¨ 2:00 åŸ·è¡Œæ•¸æ“šæ­¸æª”"
fi

echo "ğŸ“‹ ç•¶å‰ cron ä»»å‹™:"
crontab -l 2>/dev/null | grep -E "(data_archival|#)" || echo "æ²’æœ‰ç›¸é—œä»»å‹™"
```

---

## ğŸ” Phase 4: æœå‹™å±¤é©é…

### 4.1 çµ±ä¸€æŸ¥è©¢æœå‹™

å‰µå»º `backend/app/services/dual_database_service.py`ï¼š

```python
"""
é›™è³‡æ–™åº«çµ±ä¸€æŸ¥è©¢æœå‹™
"""
from datetime import datetime, timedelta
from sqlalchemy import and_, or_
from ..extensions import db
from ..models import Message, Post, StockPrice
from ..models_cold import MessageArchive, PostArchive, StockPriceHistory


class DualDatabaseService:
    """é›™è³‡æ–™åº«çµ±ä¸€æŸ¥è©¢æœå‹™"""
    
    @staticmethod
    def get_messages_unified(conversation_id, limit=50, offset=0):
        """çµ±ä¸€æŸ¥è©¢èŠå¤©è¨˜éŒ„ (ç†±åº« + å†·åº«)"""
        # å…ˆæŸ¥è©¢ç†±åº«
        hot_messages = Message.query.filter_by(
            conversation_id=conversation_id
        ).order_by(Message.created_at.desc()).limit(limit).offset(offset).all()
        
        # å¦‚æœç†±åº«æ•¸æ“šä¸è¶³ï¼ŒæŸ¥è©¢å†·åº«
        if len(hot_messages) < limit:
            remaining = limit - len(hot_messages)
            cold_messages = MessageArchive.query.filter_by(
                conversation_id=conversation_id
            ).order_by(MessageArchive.created_at.desc()).limit(remaining).all()
            
            # åˆä½µçµæœ
            all_messages = [msg.to_dict() for msg in hot_messages] + \
                          [msg.to_dict() for msg in cold_messages]
        else:
            all_messages = [msg.to_dict() for msg in hot_messages]
        
        # æŒ‰æ™‚é–“æ’åº
        all_messages.sort(key=lambda x: x['created_at'], reverse=True)
        return all_messages
    
    @staticmethod
    def get_stock_prices_unified(stock_id, start_date=None, end_date=None):
        """çµ±ä¸€æŸ¥è©¢è‚¡åƒ¹æ•¸æ“š"""
        if not start_date:
            start_date = datetime.utcnow() - timedelta(days=365)
        if not end_date:
            end_date = datetime.utcnow()
        
        # æŸ¥è©¢ç†±åº« (æœ€è¿‘30å¤©)
        hot_cutoff = datetime.utcnow() - timedelta(days=30)
        hot_prices = StockPrice.query.filter(
            and_(
                StockPrice.stock_id == stock_id,
                StockPrice.trade_date >= start_date.date(),
                StockPrice.trade_date <= end_date.date(),
                StockPrice.trade_date >= hot_cutoff.date()
            )
        ).order_by(StockPrice.trade_date.desc()).all()
        
        # æŸ¥è©¢å†·åº« (30å¤©ä»¥å‰)
        cold_prices = StockPriceHistory.query.filter(
            and_(
                StockPriceHistory.stock_id == stock_id,
                StockPriceHistory.trade_date >= start_date.date(),
                StockPriceHistory.trade_date <= end_date.date(),
                StockPriceHistory.trade_date < hot_cutoff.date()
            )
        ).order_by(StockPriceHistory.trade_date.desc()).all()
        
        # åˆä½µçµæœ
        all_prices = [price.to_dict() for price in hot_prices] + \
                    [price.to_dict() for price in cold_prices]
        
        # æŒ‰æ—¥æœŸæ’åº
        all_prices.sort(key=lambda x: x['trade_date'], reverse=True)
        return all_prices
    
    @staticmethod
    def get_posts_unified(limit=20, offset=0):
        """çµ±ä¸€æŸ¥è©¢è²¼æ–‡"""
        # å…ˆæŸ¥è©¢ç†±åº«
        hot_posts = Post.query.order_by(
            Post.created_at.desc()
        ).limit(limit).offset(offset).all()
        
        # å¦‚æœéœ€è¦æ›´å¤šæ•¸æ“šï¼ŒæŸ¥è©¢å†·åº«
        if len(hot_posts) < limit:
            remaining = limit - len(hot_posts)
            cold_posts = PostArchive.query.order_by(
                PostArchive.created_at.desc()
            ).limit(remaining).all()
            
            all_posts = [post.to_dict() for post in hot_posts] + \
                       [post.to_dict() for post in cold_posts]
        else:
            all_posts = [post.to_dict() for post in hot_posts]
        
        return all_posts
    
    @staticmethod
    def search_messages_unified(conversation_id, keyword):
        """è·¨åº«æœç´¢èŠå¤©è¨˜éŒ„"""
        # æœç´¢ç†±åº«
        hot_results = Message.query.filter(
            and_(
                Message.conversation_id == conversation_id,
                Message.content.contains(keyword)
            )
        ).all()
        
        # æœç´¢å†·åº«
        cold_results = MessageArchive.query.filter(
            and_(
                MessageArchive.conversation_id == conversation_id,
                MessageArchive.content.contains(keyword)
            )
        ).all()
        
        # åˆä½µçµæœ
        all_results = [msg.to_dict() for msg in hot_results] + \
                     [msg.to_dict() for msg in cold_results]
        
        # æŒ‰æ™‚é–“æ’åº
        all_results.sort(key=lambda x: x['created_at'], reverse=True)
        return all_results
```

### 4.2 API ç«¯é»æ›´æ–°

æ›´æ–° `backend/app/blueprints/chat.py`ï¼š

```python
from ..services.dual_database_service import DualDatabaseService

@chat_bp.route('/conversations/<int:conversation_id>/messages', methods=['GET'])
@token_required
def get_messages(current_user, conversation_id):
    """ç²å–èŠå¤©è¨˜éŒ„ (çµ±ä¸€æŸ¥è©¢ç†±åº«å’Œå†·åº«)"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)
    offset = (page - 1) * per_page
    
    try:
        # ä½¿ç”¨çµ±ä¸€æŸ¥è©¢æœå‹™
        messages = DualDatabaseService.get_messages_unified(
            conversation_id, limit=per_page, offset=offset
        )
        
        return jsonify({
            'success': True,
            'messages': messages,
            'page': page,
            'per_page': per_page,
            'total': len(messages)
        }), 200
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'ç²å–èŠå¤©è¨˜éŒ„å¤±æ•—: {str(e)}'
        }), 500
```

---

## ğŸ§ª Phase 5: æ¸¬è©¦å’Œé©—è­‰

### 5.1 å‰µå»ºæ¸¬è©¦è…³æœ¬

å‰µå»º `backend/tests/test_dual_database.py`ï¼š

```python
"""
é›™è³‡æ–™åº«æ¸¬è©¦
"""
import unittest
from datetime import datetime, timedelta
from app import create_app
from app.extensions import db
from app.models import Message, StockPrice
from app.models_cold import MessageArchive, StockPriceHistory
from app.services.dual_database_service import DualDatabaseService


class TestDualDatabase(unittest.TestCase):
    
    def setUp(self):
        self.app = create_app('testing')
        self.app_context = self.app.app_context()
        self.app_context.push()
        db.create_all()
    
    def tearDown(self):
        db.session.remove()
        db.drop_all()
        self.app_context.pop()
    
    def test_hot_cold_data_separation(self):
        """æ¸¬è©¦å†·ç†±æ•¸æ“šåˆ†é›¢"""
        # å‰µå»ºç†±æ•¸æ“š
        hot_date = datetime.utcnow() - timedelta(days=10)
        hot_message = Message(
            conversation_id=1,
            sender_id=1,
            content="ç†±æ•¸æ“šæ¸¬è©¦",
            created_at=hot_date
        )
        db.session.add(hot_message)
        
        # å‰µå»ºå†·æ•¸æ“š
        cold_date = datetime.utcnow() - timedelta(days=40)
        cold_message = MessageArchive(
            original_id=999,
            conversation_id=1,
            sender_id=1,
            content="å†·æ•¸æ“šæ¸¬è©¦",
            created_at=cold_date
        )
        db.session.add(cold_message)
        db.session.commit()
        
        # æ¸¬è©¦çµ±ä¸€æŸ¥è©¢
        messages = DualDatabaseService.get_messages_unified(1)
        self.assertEqual(len(messages), 2)
        
        # é©—è­‰æ•¸æ“šä¾†æº
        hot_found = any(msg['content'] == "ç†±æ•¸æ“šæ¸¬è©¦" for msg in messages)
        cold_found = any(msg['content'] == "å†·æ•¸æ“šæ¸¬è©¦" and msg.get('from_archive') for msg in messages)
        
        self.assertTrue(hot_found)
        self.assertTrue(cold_found)
    
    def test_database_binding(self):
        """æ¸¬è©¦æ•¸æ“šåº«ç¶å®š"""
        # æ¸¬è©¦ç†±åº«ç¶å®š
        self.assertEqual(Message.__bind_key__, None)  # é»˜èªåº«
        
        # æ¸¬è©¦å†·åº«ç¶å®š
        self.assertEqual(MessageArchive.__bind_key__, 'cold')
        self.assertEqual(StockPriceHistory.__bind_key__, 'cold')


if __name__ == '__main__':
    unittest.main()
```

---

## ğŸ“Š ç›£æ§å’Œç¶­è­·

### ç›£æ§æŒ‡æ¨™
1. **ç†±åº«æ€§èƒ½**: æŸ¥è©¢éŸ¿æ‡‰æ™‚é–“ < 100ms
2. **å†·åº«å­˜å„²**: å£“ç¸®ç‡å’Œå­˜å„²ä½¿ç”¨ç‡
3. **æ•¸æ“šé·ç§»**: æ¯æ—¥æ­¸æª”æˆåŠŸç‡
4. **è·¨åº«æŸ¥è©¢**: çµ±ä¸€æŸ¥è©¢æ€§èƒ½

### ç¶­è­·ä»»å‹™
1. **æ¯æ—¥**: è‡ªå‹•æ•¸æ“šæ­¸æª”
2. **æ¯é€±**: å†·åº«å£“ç¸®å’Œçµ±è¨ˆæ›´æ–°
3. **æ¯æœˆ**: æ€§èƒ½åˆ†æå’Œå„ªåŒ–å»ºè­°

---

## ğŸ¯ æˆåŠŸæŒ‡æ¨™

- âœ… ç†±åº«æŸ¥è©¢éŸ¿æ‡‰æ™‚é–“æå‡ 50%
- âœ… å­˜å„²æˆæœ¬é™ä½ 30%
- âœ… å‚™ä»½æ™‚é–“ç¸®çŸ­ 60%
- âœ… ç³»çµ±å¯æ“´å±•æ€§é¡¯è‘—æå‡

---

## ğŸ“ æ³¨æ„äº‹é …

1. **æ•¸æ“šä¸€è‡´æ€§**: ç¢ºä¿æ­¸æª”éç¨‹ä¸­çš„äº‹å‹™ä¸€è‡´æ€§
2. **å›æ»¾è¨ˆåŠƒ**: æº–å‚™ç·Šæ€¥å›æ»¾åˆ°å–®åº«æ¨¡å¼çš„æ–¹æ¡ˆ
3. **ç›£æ§å‘Šè­¦**: è¨­ç½®é—œéµæŒ‡æ¨™çš„ç›£æ§å‘Šè­¦
4. **æ€§èƒ½æ¸¬è©¦**: å®šæœŸé€²è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦

é€™å€‹å¯¦æ–½æŒ‡å—æä¾›äº†å®Œæ•´çš„é›™è³‡æ–™åº«æ¶æ§‹å¯¦æ–½è·¯å¾‘ï¼Œç¢ºä¿å¹³ç©©éæ¸¡å’Œé«˜æ•ˆé‹è¡Œã€‚ 